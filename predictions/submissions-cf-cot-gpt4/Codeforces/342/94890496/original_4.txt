This program is using the centroid decomposition of a tree and Lowest Common Ancestor (LCA) algorithm. The program is designed to handle big data, and it is already well optimized in terms of time complexity. However, there are few places where we can make some modifications to slightly improve its running time.

1. Use vector instead of set for adjacency list representation of the graph: The program uses set to represent the adjacency list of the graph. Sets in C++ are usually implemented as binary search trees and have log(n) time complexity for insertions and deletions. However, in this case, the graph is unweighted and simple, so we can use vector which has constant time complexity for insertions. This will decrease the time complexity of the operations performed on the graph.

2. Avoid using "bits/stdc++.h": This header file includes quite a large number of standard C++ libraries, which increases the compilation time. It's better to include only those libraries which are actually used in the program.

3. Avoid using macros for common functions: Macros like 'pb(a)', 'mp(a, b)', 'mem1(a)', 'mem0(a)' are used in the program to shorten the code. But, macros are processed by preprocessor which can slightly increase the compilation time. So, it's better to use the actual functions instead of macros.

4. Avoid using 'auto' in for loop: 'auto' automatically deduces the data type of the variable, which can increase the compilation time. So, it's better to use the actual data type.

Here's the optimized version of the program:

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <cstring>
#include <algorithm>
#define ll long long int
using namespace std;
vector<ll> v[100001], ct[100001];
ll sub[100001], nc, cr, lca[100001][21], cpar[100001], lv[100001], n, m,
    ans[100001];
void dfs(ll node, ll par, ll lvl) {
  lv[node] = lvl;
  lca[node][0] = par;
  for (ll a : v[node])
    if (a != par)
      dfs(a, node, lvl + 1);
}
void genlca() {
  memset(lca, -1, sizeof(lca));
  dfs(1, -1, 0);
  for (ll i = 1; i <= 20; i++)
    for (ll j = 1; j <= n; j++)
      if (lca[j][i - 1] != -1)
        lca[j][i] = lca[lca[j][i - 1]][i - 1];
}
ll getlca(ll a, ll b) {
  if (lv[a] > lv[b])
    swap(a, b);
  ll d = lv[b] - lv[a];
  while (d > 0) {
    ll x = log2(d);
    b = lca[b][x];
    d -= (1 << x);
  }
  if (a == b)
    return a;
  for (ll i = 20; i >= 0; i--)
    if (lca[a][i] != lca[b][i])
      a = lca[a][i], b = lca[b][i];
  return lca[a][0];
}
ll dist(ll a, ll b) { return lv[a] + lv[b] - 2 * lv[getlca(a, b)]; }
void dfs(ll node, ll par) {
  nc++;
  sub[node] = 1;
  for (ll a : v[node])
    if (a != par)
      dfs(a, node), sub[node] += sub[a];
}
ll getcent(ll node, ll par) {
  for (ll a : v[node])
    if (a != par && sub[a] > nc / 2)
      return getcent(a, node);
  return node;
}
ll decom(ll root) {
  nc = 0;
  dfs(root, -1);
  ll c = getcent(root, -1);
  for (ll a : v[c]) {
    v[a].erase(remove(v[a].begin(), v[a].end(), c), v[a].end());
    ll x = decom(a);
    ct[c].push_back(x);
    ct[x].push_back(c);
    cpar[x] = c;
  }
  return c;
}
void update(ll par, ll node) {
  if (par == -1)
    return;
  ans[par] = min(ans[par], dist(node, par));
  update(cpar[par], node);
}
ll getans(ll par, ll node) {
  if (par == -1)
    return INT_MAX;
  return min(getans(cpar[par], node), ans[par] + dist(node, par));
}
int main() {
  cin >> n >> m;
  for (ll i = 1; i < n; i++) {
    ll a, b;
    cin >> a >> b;
    v[a].push_back(b);
    v[b].push_back(a);
  }
  for (ll i = 0; i <= n; i++)
    ans[i] = INT_MAX;
  genlca();
  cr = decom(1);
  cpar[cr] = -1;
  ans[1] = 0;
  update(cpar[1], 1);
  while (m--) {
    ll a, b;
    cin >> a >> b;
    if (a == 1) {
      ans[b] = 0;
      update(cpar[b], b);
    } else {
      cout << getans(b, b) << "\n";
    }
  }
  return 0;
}
```

Please note that this optimization will not drastically improve the performance of the program. The original program was already well optimized for handling large data in terms of time complexity. These changes are minor and would only make a slight difference in running time.