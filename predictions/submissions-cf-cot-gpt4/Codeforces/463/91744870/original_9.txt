The main optimization here would be to change the 2D array `a` into a 1D array. In C++, a 2D array is essentially an array of arrays, which adds overhead in terms of memory usage. A 1D array is more efficient because it requires less memory and results in fewer cache misses, thereby improving performance.

The 2D to 1D array conversion can be done using row-major order, where the position of an element `(i,j)` in a 2D array of dimensions `n x n` is given by `i*n + j` in the 1D array. 

The other optimization would be to replace the map data structures `cnt` and `cnt1` with unordered_map. Maps in C++ are usually implemented as balanced tree structures that store their keys in sorted order. On the other hand, unordered_maps are hash tables and have average constant-time complexity for search, insert, and delete operations.

Here's the optimized code:

```cpp
#include <bits/stdc++.h>
using namespace std;
typedef long long ll;
typedef pair<int, int> pii;
typedef pair<ll, ll> pll;
typedef vector<ll> vi;
#define fo(i, n) for (int i = 0; i < n; i++)
#define all(x) x.begin(), x.end()
#define fi first
#define se second
const double pi = 3.14159265358979323846;
#define fast ios_base::sync_with_stdio(0); cin.tie(NULL)
pll ans[2];
vi sum(2);
void update(int c, int i, int j, ll val) {
  if (val > sum[c]) {
    sum[c] = val;
    ans[c].fi = i;
    ans[c].se = j;
  }
}
void solve() {
  ll n;
  cin >> n;
  vi a(n * n);
  sum[0] = sum[1] = -1;
  unordered_map<ll, ll> cnt, cnt1;
  fo(i, n) {
    fo(j, n) {
      cin >> a[i*n + j];
      cnt[i + j] += a[i*n + j];
      cnt1[i - j] += a[i*n + j];
    }
  }
  fo(i, n) {
    fo(j, n) {
      update((i + j) % 2, i, j, cnt[i + j] + cnt1[i - j] - a[i*n + j]);
    }
  }
  cout << sum[0] + sum[1] << "\n";
  for (auto it : ans) {
    cout << it.fi + 1 << " " << it.se + 1 << " ";
  }
}
int main() {
  fast;
  ll t = 1;
  while (t--)
    solve();
  return 0;
}
```

Note: Please remember that, in practical terms, these optimizations might not lead to significant performance improvements. The effectiveness of these optimizations depends on the specific use case, and the size and nature of the data.