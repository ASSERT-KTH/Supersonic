1. Code Analysis: The given C++ program is an implementation of the suffix array data structure. Suffix arrays are a data structure used for efficient string operations. The program follows a standard implementation of suffix arrays, including functions to construct and query the data structure.

2. Optimization Strategy:

    A. Replacing inefficient data structures: The data structures in the code seem to be the standard ones used in C++ and are efficient for the operations being performed. There are no unnecessary data structures that can be replaced or optimized.

    B. Reducing redundant computations: The code seems to be free of redundant computations. The for-loops are necessary for the construction and handling of the suffix arrays and their corresponding lcp arrays.

    C. Optimizing loops or recursive calls: The loops in the code are necessary for the construction and handling of the suffix arrays. However, some of these loops can be parallelized to improve performance.

    D. Compiler optimizations or pragma directives: The code can be optimized using OpenMP, a compiler directive used to parallelize loops in C++. This will speed up the construction of the suffix array and lcp array.

3. Step-by-Step Explanation:

    A. Parallelizing for-loops: For-loops in the code can be parallelized to improve performance. For instance, the for-loop in the "construct_lcp()" function can be parallelized. This will divide the work among multiple cores, speeding up the construction of the lcp array. However, this might increase the complexity of the code and make it harder to debug.

    B. Compiler optimizations: Using the "-O3" flag when compiling the code will enable all compiler optimizations, which might improve performance.

4. Optimized Code Block: The optimized code is as follows. Note that the changes are minimal, but they should improve the performance of the code.

```cpp
#include <bits/stdc++.h>
#include <omp.h>
using namespace std;

// ... other code unchanged ...

void construct_lcp() {
    lcp.resize(s.size());
    rank.resize(s.size() + 1);
    int n = s.size();
    for (int i = 0; i <= n; i++)
      rank[sa[i]] = i;
    int h = 0;
    lcp[0] = 0;
    #pragma omp parallel for
    for (int i = 0; i < n; i++) {
      int j = sa[rank[i] - 1];
      if (h > 0)
        h--;
      for (; j + h < n && i + h < n; h++) {
        if (s[j + h] != s[i + h])
          break;
      }
      lcp[rank[i] - 1] = h;
    }
}

// ... other code unchanged ...

signed main() {
  ios_base::sync_with_stdio(false); 
  cin.tie(NULL); 

  string s;
  cin >> s;
  suffix_array sa(s);
  int q;
  cin >> q;
  while (q--) {
    string t;
    cin >> t;
    cout << sa.contain(t) << endl;
  }
  return 0;
}
```

When compiling the code, use the "-O3" flag to enable all compiler optimizations and "-fopenmp" to enable OpenMP.

```
g++ -O3 -fopenmp main.cpp -o main
```

Note: The OpenMP parallelization may not always lead to a speedup (especially on a single-core machine or if the workload is too small to offset the overhead of parallelization). Therefore, it is recommended to profile the code before and after the optimization to measure the actual performance improvement.